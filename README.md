### ENG :
# AI Doctor Assistant

### ===== ğŸ©º About the Project

This project is an LLM-based doctor assistant developed using LangChain, FastAPI, and Chainlit, leveraging Groq via Chainlit or Ollama locally.
It extracts symptoms, manages memory, and performs evaluations from user messages written in natural language (Turkish or English).

Short Architecture Overview
    > User â†’ Chainlit â†’ FastAPI â†’ LangChain â†’ LLM (Groq or Ollama)

### ===== âš™ï¸ Main Features

##### ğŸŒ Multilingual Support (Turkish / English)
    > Automatically detects the userâ€™s language.  
    > Memory is preserved even if the language changes mid-conversation.  
    > Custom functions are implemented for language detection and control.

##### ğŸ§  Dynamic Memory Management
    > Previous messages are stored using InMemoryChatMessageHistory.  
    > LLM responses are generated contextually.

##### ğŸ§© Symptom Extraction and Evaluation
    > Symptoms are extracted from messages, triggering the evaluation workflow.

##### âš•ï¸ Evaluation Workflow
    > Analyzes recorded symptoms to provide urgency, recommendations, and risk assessment.

##### ğŸ§¬ Asynchronous Architecture (async/await)
    > The backend operates asynchronously across all flows. Fast, modular, and scalable.


### ===== ğŸ§© Technologies Used

##### ğŸ–¥ï¸ Frontend 
    > Chainlit interface: Interactive chat UI  
    > Action Callbacks: Button-based interactions like "Assess", "Show Summary", "Clear"  
    > Real-time Session Management: User-specific session handling

##### ğŸ§© Backend

| Technology                | Description                                |
| ------------------------- | ------------------------------------------ |
| **FastAPI**               | Asynchronous REST API service              |
| **LangChain**             | LLM flow management and memory             |
| **ChatGroq / ChatOllama** | LLM models (local or cloud-based)          |
| **Pydantic**              | Data validation and schema management      |
| **Custom Memory Manager** | User-specific memory control               |
| **Language Detector**     | Automatic message-based language detection |


### ===== Why Two Different Models? =====

While Ollama is the most suitable model for this project, for demo purposes and to allow everyone to try it, we used Groq via Chainlit.
Additionally, to demonstrate real-world usage, Ollama can be run locally.

**Note:** If you remove the Groq part from llm.py, you can continue to use Ollama + Chainlit locally. You only need to remove the Groq model.

### ===== ğŸš€ Setup and Running

**First, create a virtual environment and install dependencies:**

> python -m venv venv
> venv\Scripts\activate
> pip install -r requirements.txt


**If you want to use Chainlit + Groq:**

Open two terminals to run the project:    
1. Start the API in one terminal:
   > uvicorn app.api.assistant_api:app --reload
2. Start Chainlit in another terminal (choose any free port, e.g., 8002):
   > chainlit run main.py --port 8002 -wh

**Note:** Since FastAPI uses port 8000, Chainlit should run on a different port.

**If you want to use Chainlit + Ollama:**

Open three terminals to run the project and chat with the LLM:
1. Start the Ollama service:
   > ollama serve
2. Start the API:
   > uvicorn app.api.assistant_api:app --reload
3. Run the client test script:
   > python C:\...\DoctorAssistant\tests\client_test.py
(Replace with your own client_test.py absolute path. Right-click the file â†’ Copy Path/Reference â†’ Absolute Path.)
    
**Note**: To use the Ollama model, Ollama must be installed on your machine, and the model defined in config.py (e.g., "qwen2.5:3b") must be downloaded.
    
    You can use any model; alternatives are listed in config.py. Just ensure itâ€™s installed in Ollama before use.


#### ===========  ******** =========  ******** ========= ******** =========

### TR :
# AI Doktor AsistanÄ±

### ===== ğŸ©º Proje HakkÄ±nda

Bu proje, LangChain, FastAPI, Chainlit' de Groq/ localde Ollama ve Chainlit kullanÄ±larak geliÅŸtirilmiÅŸ bir LLM tabanlÄ± doktor asistanÄ±dÄ±r.
KullanÄ±cÄ±nÄ±n doÄŸal dilde (TÃ¼rkÃ§e veya Ä°ngilizce) yazdÄ±ÄŸÄ± mesajlardan semptom Ã§Ä±karÄ±mÄ±, hafÄ±za yÃ¶netimi ve deÄŸerlendirme yapar.

Mimarinin KÄ±sa Ã–zeti
    > KullanÄ±cÄ± â†’ Chainlit â†’ FastAPI â†’ LangChain â†’ LLM (Groq veya Ollama)

### ===== âš™ï¸ Ana Ã–zellikler

##### ğŸŒ Ã‡ok Dilli Destek (TÃ¼rkÃ§e / Ä°ngilizce)
    > KullanÄ±cÄ±nÄ±n konuÅŸma dili otomatik tespit edilir.
    > Dil konuÅŸma ortasÄ±nda deÄŸiÅŸse bile hafÄ±za korunur.
    > Dil tespiti ve kontrolleri iÃ§in fonksiyonlar yazÄ±lmÄ±ÅŸtÄ±r

##### ğŸ§  Dinamik HafÄ±za YÃ¶netimi
    > InMemoryChatMessageHistory kullanÄ±larak Ã¶nceki mesajlar saklanÄ±r.
    > LLM cevaplarÄ± baÄŸlama gÃ¶re Ã¼retir.

##### ğŸ§© Semptom Ã‡Ä±karÄ±mÄ± ve DeÄŸerlendirme
    > Mesajlardan semptomlar Ã§Ä±karÄ±lÄ±r ve deÄŸerlendirme akÄ±ÅŸÄ± baÅŸlatÄ±lÄ±r.

##### âš•ï¸ DeÄŸerlendirme AkÄ±ÅŸÄ±
    > KayÄ±tlÄ± semptomlarÄ± analiz ederek aciliyet, Ã¶neri ve risk deÄŸerlendirmesi sunar.

##### ğŸ§¬ Asenkron YapÄ± (async/await)
    > Backend tÃ¼m akÄ±ÅŸlarda asenkron Ã§alÄ±ÅŸÄ±r. HÄ±zlÄ±, modÃ¼ler ve Ã¶lÃ§eklenebilirdir

### ===== ğŸ§© KullanÄ±lan Teknolojiler

##### ğŸ–¥ï¸ Frontend 
    > Chainlit arayÃ¼zÃ¼ : EtkileÅŸimli sohbet arayÃ¼zÃ¼
    > Action Callbackâ€™ler : â€œDeÄŸerlendirâ€, â€œÃ–zet GÃ¶râ€, â€œTemizleâ€ gibi buton tabanlÄ± etkileÅŸimler
    > GerÃ§ek ZamanlÄ± Oturum YÃ¶netimi : KullanÄ±cÄ±ya Ã¶zel session yÃ¶netimi

##### ğŸ§© Backend:

| Teknoloji                 | AÃ§Ä±klama                                 |
| ------------------------- | ---------------------------------------- |
| **FastAPI**               | Asenkron REST API servisi                |
| **LangChain**             | LLM akÄ±ÅŸ yÃ¶netimi ve hafÄ±za              |
| **ChatGroq / ChatOllama** | LLM modelleri (local veya bulut tabanlÄ±) |
| **Pydantic**              | Veri doÄŸrulama ve ÅŸema yÃ¶netimi          |
| **Custom Memory Manager** | KullanÄ±cÄ± bazlÄ± bellek kontrolÃ¼          |
| **Language Detector**     | Mesaj bazlÄ± otomatik dil algÄ±lama        |


### ===== Neden iki FarklÄ± Model KullandÄ±m? =====

Bu projenin konusuna en uygun yÃ¶ntem Ollama modeli kullanmaktÄ±r ama bu bir demo projesi olduÄŸu iÃ§in herkesin deneyebilmesi adÄ±na Chainlit ile Groq modelini kullandÄ±m.
AyrÄ±ca, bÃ¶yle bir projenin gerÃ§ek hayatta nasÄ±l kullanÄ±labileceÄŸini gÃ¶stermek iÃ§in de yerel makinede ollama kullandÄ±m.
Not: llm.py de groq kÄ±smÄ±nÄ± kaldÄ±rÄ±rsanÄ±z ollama + chainlit yapÄ±sÄ±yla lokalde kullanmaya devam edebilirsiniz. Sadece Groq modelini kaldÄ±rmanÄ±z yeterli.


### ===== ğŸš€ Kurulum ve Ã‡alÄ±ÅŸtÄ±rma

**Ã–ncelikle sanal ortamÄ± ayarlayÄ±n ve baÄŸÄ±mlÄ±lÄ±klarÄ± yÃ¼kleyin.**

> python -m venv venv
> venv\Scripts\activate
> pip install -r requirements.txt

**Projede Chainlit + Groq kullanmak isterseniz;**

Projeyi Ã§alÄ±ÅŸtÄ±rmak iÃ§in iki terminal aÃ§malÄ±sÄ±nÄ±z
        
1. terminale api baÅŸlatma komutunu yazmalÄ±sÄ±nÄ±z
   > uvicorn app.api.assistant_api:app --reload
2. terminale chainlit baÅŸlatma komutunu yazmalÄ±sÄ±nÄ±z (boÅŸ olan bir port girebilirsiniz, ben 8002 portunu kullandÄ±m)
   > chainlit run main.py --port 8002 -wh

   **Not**: fast api 8000 portunu kullandÄ±ÄŸÄ± iÃ§in chainlit iÃ§in farklÄ± bir port girmeliyiz

=========

**EÄŸer Chainlit + Ollama kullanmak isterseniz;**

Projeyi Ã§alÄ±ÅŸtÄ±rmak ve llm ile sohbet etmek iÃ§in Ã¼Ã§ terminal aÃ§malÄ±sÄ±nÄ±z
1. terminale aÅŸaÄŸÄ±daki ollama servisini baÅŸlatan kodu yazmalÄ±sÄ±nÄ±z
   > ollama serve
2. terminale api baÅŸlatma komutunu yazmalÄ±sÄ±nÄ±z
   > uvicorn app.api.assistant_api:app --reload
3. terminale client_test yolunu yazmalÄ±sÄ±nÄ±z ve artÄ±k Ã§alÄ±ÅŸtÄ±rabilirsiniz
   > python C:\...\DoctorAssistant\tests\client_test.py  (kendi client_test yolunuzu girin. Bunu yapmak iÃ§in;
client_test dosyayÄ±nzÄ±n Ã¼zerine saÄŸ tÄ±klayÄ±n "Copy Path/Reference" ye tÄ±klayÄ±n ve ardÄ±ndan "Absolute Path' e tÄ±klamanÄ±z yeterli. Yolu kopyalacaktÄ±r.")

**Not**: Ollama modelinin Ã§alÄ±ÅŸmasÄ± iÃ§in bilgisayarÄ±nÄ±zda ollama'nÄ±n kurulu olmasÄ± ve ollama iÃ§inde de config.py' de belirlediÄŸimiz "qwen2.5:3b" modelinin yÃ¼klÃ¼ olmasÄ± gerekmektedir!
Ä°stediÄŸiniz herhangi bir model kullanabilirsiniz alternatif modelleri cofig.py' de belirttim. Sadece kullanmadan Ã¶nce Ollama'da yÃ¼klÃ¼ olduÄŸundan emin olun!


### ğŸ§‘â€ğŸ’» Developer ( GeliÅŸtirici )
ğŸ‘‹ **GÃ¼lÅŸen Demir** â€” Data Scientist & AI Developer

### ===== ğŸ“« Contact ( Ä°letiÅŸim )
    Email: alyula.coder@gmail.com | gulsendemirbm@gmail.com
    LinkedIn: linkedin.com/in/gulsendemir

### ===== â–¶ï¸ Project Demo and Video Presentation ( Projenin Demo ve Video Sunumu )

Chainlit-App : []
Youtube: [https://youtu.be/P3jrZfgo25w]

